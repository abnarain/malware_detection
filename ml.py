import matplotlib.pyplot as plt
from sklearn.cross_validation import train_test_split,KFold, cross_val_score 

import scipy
from scipy import stats
import numpy as np


#probabilities for the predictions, use predict_proba() and see the content of the classes_
def coeff_of_deterimination(classifier, X, y, K=10):
    # Perform a cross-validation estimate of the coefficient of determination using
    # the cross_validation module using all CPUs available on the machine
    R2 = cross_val_score(classifier, X, y=y, cv=KFold(y.size, K), n_jobs=1).mean()
    print "The %d-Folds est coeff. of determ. R2 = %s" % (K, R2)

def roc_plot(X,y, classifier,filename):
    from sklearn.metrics import roc_curve, auc
    from sklearn.cross_validation import StratifiedKFold
    plt.figure(figsize=(10,9))
    mean_tpr = 0.0
    mean_fpr = np.linspace(0, 1, 100)
    all_tpr = []
    cv = StratifiedKFold(y, n_folds=5)
    for i, (train, test) in enumerate(cv):
        probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])
    # Compute ROC curve and area the curve
        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])
        mean_tpr += scipy.interp(mean_fpr, fpr, tpr)
        mean_tpr[0] = 0.0
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))

        
    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')

    mean_tpr /= len(cv)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    plt.plot(mean_fpr, mean_tpr, 'k--',  label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)
    
    plt.xlim([-0.05, 1.05])
    plt.ylim([-0.05, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic')
    plt.legend(loc="lower right")
    plt.savefig(filename+'.pdf')


def plot_conf_matrix(cm, title, filename):
    plt.figure(figsize=(8,9))
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(title)
    plt.colorbar()
    target_names=['len', 'min', 'max', 'mean', 'var', 'skew', 'kurt' , 'iqr']
    tick_marks = np.arange(len(target_names))
    plt.xticks(tick_marks, target_names, rotation=45)
    plt.yticks(tick_marks, target_names)
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.savefig(filename+'.pdf')

def calc_conf_matrix(y_true, y_pred, title, filename):
    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import r2_score 
    cm =confusion_matrix(y_true, y_pred)
    print 'Confusion matrix(without normalization)' 
    print cm 
    print "R2_score (r2_score) ", r2_score(y_true, y_pred, multioutput='variance_weighted') 
    plot_conf_matrix(cm, title, filename)

    '''
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    print 'Normalized confusion matrix'
    print cm_normalized 
    plot_conf_matrix(cm_normalized,'Normalized confusion matrix','normalized_conf_matrix')
    '''


def crossValidation(input_data, output_labels, classifier_to_test):
    return 0
    from sklearn.cross_validation import StratifiedKFold
    from sklearn.metrics import zero_one_loss
    k = 3
    skf = StratifiedKFold(output_labels,n_folds=k)
    averageError = 0.0
    for train_index, test_index in skf:
        X_train, X_test = input_data[:,train_index], input_data[:,test_index]
        Y_train, Y_test = output_labels[train_index], output_labels[test_index]
        classifier_to_test.fit(X_train,Y_train)
        #classifier_to_test.fit(X_train.T,Y_train)
        Y_pred = classifier_to_test.predict(X_test)
        #Y_pred = classifier_to_test.predict(X_test.T)
        error = zero_one_loss(Y_pred,Y_test)
        averageError += (1./k) * error

    print "Average error after Cross Validation : %4.2f%s" % (100 * averageError,'%')

'''
def mlpclassifier(input_data, output_labels) :
    from sklearn.neural_network import MLPClassifier
    mlpC = MLPClassifier(algorithm='l-bfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)
    crossValidation(input_data, output_labels, mlpC)
    X_train, X_test, Y_train, Y_test = train_test_split(input_data, output_labels, test_size=0.25, random_state=42)

    mlpC.fit(X_train, Y_train)
    predictionsMLP= mlpC.predict(X.test.T)
    calc_conf_matrix(Y_test, predictionsMLP, 'Multi Layer Perceptron confusion matrix', 'multi_layer_perceptron_cm')
    roc_plot(input_data,output_labels, mlpC,'roc_mlp')
    coeff_of_deterimination(mlpC, input_data, output_labels, 3)
'''


def mlpclassifier(input_data, output_labels) :
    from sknn.mlp import Classifier, Layer

    mlpC = Classifier(
        layers=[
            #Layer("Maxout", units=100, pieces=2),
            Layer("Softmax")],
            learning_rate=0.001,
            n_iter=25)
    X_train, X_test, Y_train, Y_test = train_test_split(input_data, output_labels, test_size=0.25, random_state=42)
    mlpC.fit(X_train, Y_train)
    
    predictionsMLP= mlpC.predict(X_test)
    calc_conf_matrix(Y_test, predictionsMLP, 'Multi Layer Perceptron confusion matrix', 'multi_layer_perceptron_cm')
    roc_plot(input_data,output_labels, mlpC,'roc_mlp')
    coeff_of_deterimination(mlpC, input_data, output_labels, 5)
    #print mlpC.feature_importances_




def decisiontreeclassifier(input_data, output_labels):
    from sklearn.tree import DecisionTreeClassifier
    dtC = DecisionTreeClassifier(max_depth=None, min_samples_split=1, random_state=0)
    crossValidation(input_data, output_labels, dtC)
    X_train, X_test, Y_train, Y_test = train_test_split(input_data, output_labels, test_size=0.25, random_state=42)

    dtC.fit(X_train,Y_train)
    predictionsdtC= dtC.predict(X_test)
    calc_conf_matrix(Y_test, predictionsdtC, 'Decision Tree confusion matrix', 'decision_tree_cm')
    roc_plot(input_data,output_labels, dtC,'roc_dec_tree')
    coeff_of_deterimination(dtC, input_data, output_labels, 3)
    print dtC.feature_importances_


def randforestclassifier(input_data,output_labels, n_est=4, maxDepth = 5, maxLearners = 100 ):
    from sklearn.ensemble import RandomForestClassifier
    #RF=RandomForestClassifier(n_estimators = n_est, max_depth = maxDepth, warm_start = False)
    rfC=RandomForestClassifier(n_estimators = n_est)
    crossValidation(input_data, output_labels, rfC)
    X_train, X_test, Y_train, Y_test = train_test_split(input_data, output_labels, test_size=0.25, random_state=42)

    rfC.fit(X_train,Y_train)
    predictionsrfC= rfC.predict(X_test)
    calc_conf_matrix(Y_test, predictionsrfC, 'Random Forest confusion matrix','random_forest_cm')
    roc_plot(input_data,output_labels, rfC,'roc_rand_forest')
    coeff_of_deterimination(rfC, input_data, output_labels,3)
    print rfC.feature_importances_


def extratreeclassifier(input_data,output_labels,m_d=3, n_est=10, rs=0):
    # Learn an ExtraTreesClassifier for comparison
    from sklearn.ensemble import ExtraTreesClassifier
    etC = ExtraTreesClassifier(max_depth= m_d, n_estimators=n_est, random_state=rs)
    crossValidation(input_data, output_labels, etC)
    X_train, X_test, Y_train, Y_test = train_test_split(input_data, output_labels, test_size=0.25, random_state=42)

    etC.fit(X_train,Y_train)
    predictionsTrees = etC.predict(X_test)
    calc_conf_matrix(Y_test, predictionsTrees, 'Extra Tree Classifier confusion matrix', 'extratree_cm')
    roc_plot(input_data,output_labels, etC,'xtra_tree_roc')
    coeff_of_deterimination(etC, input_data, output_labels, 3)
    print etC.feature_importances_



def knnclassifier(input_data, output_labels, k = 2):
    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier(n_neighbors=k)
    crossValidation(input_data, output_labels, knn)
    X_train, X_test, Y_train, Y_test = train_test_split(input_data, output_labels, test_size=0.25, random_state=42)

    knn.fit(X_train,Y_train)
    predictionsKNN = knn.predict(X_test)
    calc_conf_matrix(Y_test, predictionsKNN, 'KNN confusion matrix', 'knn_cm')
    roc_plot(input_data,output_labels, knn,'roc_knn')
    coeff_of_deterimination(knn, input_data, output_labels, K=3)
    #print knn.feature_importances_

    
def kmeans(X):
    from sklearn import KMeans
    k_means = KMeans(init='k-means++', n_clusters=2, n_init=10)
    k_means.fit(X)
    k_means_labels = k_means.labels_
    k_means_cluster_centers = k_means.cluster_centers_
    k_means_labels_unique = np.unique(k_means_labels)

def svmclassifier(X,y):
    from sklearn import svm
    SVM= svm.SVC(kernel='linear', probability=True, random_state=random_state)


def dimension_reduction(X, ncomponents=2):
    pca = TruncatedSVD(n_components)
    X_reduced = pca.fit_transform(X)
    return X_reduced


def naive_bayes(X,Y):
    # Learn a Naive Bayes classifier on the transformed data
    nb = BernoulliNB()
    nb.fit(X, Y)


if __name__=='__main__':
    print "main" 
    import sys
    msg_iq_file= sys.argv[1]
    noise_iq_file=sys.argv[2]
    print "msg file ", msg_iq_file
    print "noise files", noise_iq_file

    msg_iq_data= scipy.fromfile(open(msg_iq_file), dtype=scipy.complex64)
    msg_iq_data= msg_iq_data[0:50000]
    msg_mod  =map(np.absolute,msg_iq_data)
    msg_mag = map(lambda x: x**2, msg_mod)

    noise_iq_data= scipy.fromfile(open(noise_iq_file), dtype=scipy.complex64)
    noise_iq_data= noise_iq_data[0:50000]
    noise_mod=map(np.absolute,noise_iq_data)
    noise_mag=map(lambda x: x**2, noise_mod)
    chunks = lambda lst, sz: [lst[i:i+sz] for i in range(0, len(lst), sz)]
    
    noise_total_seconds =msg_total_seconds=400

    msg_div_len= int(5*len(msg_mag)/msg_total_seconds)
    msg_array= chunks(msg_mag,msg_div_len)

    noise_div_len= int(5*len(noise_mag)/noise_total_seconds )
    noise_array= chunks(noise_mag,noise_div_len)

    X_noise, X_msg= [], []
    Y_noise, Y_msg= [], []
    for i in range(0, len(noise_array)):
        noise_n,(noise_min, noise_max), noise_mean, noise_var, noise_skew, noise_kurt = scipy.stats.describe(noise_array[i])
        noise_iqr = np.subtract(*np.percentile(noise_mag, [75, 25]))
        if np.isnan([noise_n, noise_min, noise_max, noise_mean, noise_var, noise_skew, noise_kurt, noise_iqr]).any():
            continue
        else:
            pass
        X_noise.append( [noise_n, noise_min, noise_max, noise_mean, noise_var, noise_skew, noise_kurt, noise_iqr] )
        Y_noise.append(0)

    for i in range(0, len(msg_array)):
        msg_n, (msg_min, msg_max), msg_mean, msg_var, msg_skew, msg_kurt = scipy.stats.describe(msg_array[i])
        msg_iqr = np.subtract(*np.percentile(msg_mag, [75, 25]))
        if np.isnan([msg_n, msg_min, msg_max, msg_mean, msg_var, msg_skew, msg_kurt , msg_iqr]).any(): 
            continue
        else:
            pass
        X_msg.append([msg_n, msg_min, msg_max, msg_mean, msg_var, msg_skew, msg_kurt , msg_iqr])
        Y_msg.append(1)

    X_noise, X_msg, Y_noise, Y_msg = np.array(X_noise),np.array(X_msg),np.array(Y_noise),np.array(Y_msg)
    X_=np.concatenate((X_noise,X_msg),axis=0)
    Y_=np.concatenate((Y_noise,Y_msg),axis=0)
    n_samples, n_features = X_.shape
    print "number of samples ", n_samples, "features ", n_features
    print "KNN"
    knnclassifier(X_,Y_)
    print "Random Forest"
    randforestclassifier(X_,Y_, n_est=4, maxDepth = 5, maxLearners = 100)
    print " Multi Layer Perceptron"
    mlpclassifier(X_, Y_)
    print "Extra Trees "
    extratreeclassifier(X_,Y_)
    print "Decision Trees "
    decisiontreeclassifier(X_, Y_)


